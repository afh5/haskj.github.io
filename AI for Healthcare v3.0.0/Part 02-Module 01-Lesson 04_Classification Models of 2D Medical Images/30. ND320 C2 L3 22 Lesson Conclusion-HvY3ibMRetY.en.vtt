WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.620
Congratulations.

00:00:01.620 --> 00:00:04.395
This was a big lesson and you made it to the end.

00:00:04.395 --> 00:00:06.510
Now let's review what you learned.

00:00:06.510 --> 00:00:08.040
In the first section,

00:00:08.040 --> 00:00:09.510
we talked about the history of

00:00:09.510 --> 00:00:12.855
classic machine learning and deep learning and their differences.

00:00:12.855 --> 00:00:14.775
Then we looked at Otsu's method,

00:00:14.775 --> 00:00:17.280
a classic machine learning algorithm that can be used

00:00:17.280 --> 00:00:19.980
to extract background and make classifications.

00:00:19.980 --> 00:00:23.250
Then we talked about different deep learning architectures.

00:00:23.250 --> 00:00:24.830
In the second section,

00:00:24.830 --> 00:00:26.930
we covered how to split a data set into

00:00:26.930 --> 00:00:29.870
a balanced training set that is used to train a model,

00:00:29.870 --> 00:00:34.670
and an imbalanced validation set that's used to test model performance in the wild.

00:00:34.670 --> 00:00:38.660
Next, we talked about what a gold standard is and different ways to

00:00:38.660 --> 00:00:43.250
establish ground truth of the data as gold standards are often unavailable.

00:00:43.250 --> 00:00:44.675
At the end of this section,

00:00:44.675 --> 00:00:49.075
we introduced the silver standard derived from several radiologists.

00:00:49.075 --> 00:00:50.820
In the fourth section,

00:00:50.820 --> 00:00:53.825
we introduced different ways to preprocess images.

00:00:53.825 --> 00:00:58.705
Then we talked about different image augmentation methods to expand our training set.

00:00:58.705 --> 00:01:00.370
In the fifth section,

00:01:00.370 --> 00:01:05.110
we discussed that CNN filters can detect simple features like lines and shapes.

00:01:05.110 --> 00:01:07.850
Then we discussed how researchers find tuning

00:01:07.850 --> 00:01:13.790
pre-trained deep learning models such as VGG 16 can do this for specific use cases.

00:01:13.790 --> 00:01:15.935
Finally, in the last section,

00:01:15.935 --> 00:01:19.070
we talked about how the model is trained and how we monitor

00:01:19.070 --> 00:01:22.925
training and validation loss to decide when to stop training the model.

00:01:22.925 --> 00:01:25.550
At the end, we talked about overfitting and

00:01:25.550 --> 00:01:28.085
how to use different methods such as batch size,

00:01:28.085 --> 00:01:31.235
learning rate, and dropout to prevent overfitting.

00:01:31.235 --> 00:01:33.050
I hope you enjoyed this lesson.

00:01:33.050 --> 00:01:37.770
We are one lesson away from completing the course. See you guys in a bit.

