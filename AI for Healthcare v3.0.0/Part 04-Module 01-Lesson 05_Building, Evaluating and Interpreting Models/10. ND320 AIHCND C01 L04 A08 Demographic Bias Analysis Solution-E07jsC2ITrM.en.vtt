WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.989
For this exercise, you use a dataset from Aequitas

00:00:03.989 --> 00:00:08.865
called Compas that looked at the results from criminal risk algorithms.

00:00:08.865 --> 00:00:10.440
You are asked to perform

00:00:10.440 --> 00:00:17.670
a Fairness Disparity Analysis with using the reference group Asian females under 25.

00:00:17.670 --> 00:00:21.570
I reference a documentation for more details and gave

00:00:21.570 --> 00:00:26.130
some the key instructions about this type of analysis in the notebook.

00:00:26.130 --> 00:00:27.530
I just want to note though,

00:00:27.530 --> 00:00:30.305
there's a lot of functionality available,

00:00:30.305 --> 00:00:36.055
in Aequitas and we're just giving a snapshot of what is available for you to use.

00:00:36.055 --> 00:00:41.540
So here's some more information about the Aequitas Compas Dataset.

00:00:41.540 --> 00:00:46.970
This was a dataset from 2016 from ProPublica that looked

00:00:46.970 --> 00:00:52.220
at automated criminal risk assessment algorithms and we adapted it,

00:00:52.220 --> 00:00:55.040
the dataset from the notebook that aequitas

00:00:55.040 --> 00:00:58.670
provides and preprocessed with this script as well.

00:00:58.670 --> 00:01:04.070
Here are some more instructions about calculating disparities across reference groups.

00:01:04.070 --> 00:01:08.005
Again, this was adapted from the notebook and from the site.

00:01:08.005 --> 00:01:11.645
So now, let's move on to actually working with this data.

00:01:11.645 --> 00:01:16.430
So we read this in and here's a preview of what this dataset looks like.

00:01:16.430 --> 00:01:17.585
As you can see,

00:01:17.585 --> 00:01:18.890
the score label value,

00:01:18.890 --> 00:01:23.030
these are standard for how you use aequitas and then we have

00:01:23.030 --> 00:01:27.334
a demographic information which should be pretty clear to understand.

00:01:27.334 --> 00:01:32.290
So race, gender and they have some age bin as well.

00:01:32.290 --> 00:01:34.544
Now, we're on to the solution,

00:01:34.544 --> 00:01:36.600
in terms of data preparation,

00:01:36.600 --> 00:01:39.995
this is similar to the method we went over in the walk through.

00:01:39.995 --> 00:01:43.430
Next, we can use this data to extract some of

00:01:43.430 --> 00:01:47.555
the metrics across the dataset and groups in a summary table,

00:01:47.555 --> 00:01:52.400
there's a nice function to see the metrics in a combined visual as well.

00:01:52.400 --> 00:01:56.285
This is more to show you what can be done to explore aequitas.

00:01:56.285 --> 00:01:59.525
We'll now go to actual reference group analysis.

00:01:59.525 --> 00:02:02.045
But I did want to show you how you could easy look at

00:02:02.045 --> 00:02:05.465
different metrics across the different attributes.

00:02:05.465 --> 00:02:08.545
So for the Fairness Disparity Analysis,

00:02:08.545 --> 00:02:10.550
we need to create a reference group,

00:02:10.550 --> 00:02:16.400
in this case, we were given Asian female under 25 as the reference group.

00:02:16.400 --> 00:02:21.060
So we add that to a dictionary in this argument here and now we're ready

00:02:21.060 --> 00:02:25.760
for the next part which is to actually analyze a particular metric.

00:02:25.760 --> 00:02:30.125
In this case, we're asked to focus on false positive rate.

00:02:30.125 --> 00:02:35.945
False positive rate indicates that someone was falsely identified by the algorithm.

00:02:35.945 --> 00:02:40.670
For this plot, I just wanted to note that the reference group is always equal to

00:02:40.670 --> 00:02:45.905
one and is in gray and the other boxes are multiples of one.

00:02:45.905 --> 00:02:48.980
So what this is saying is that each group is

00:02:48.980 --> 00:02:54.475
some multiplier which is more or less likely to be falsely identified.

00:02:54.475 --> 00:02:56.270
For our example here,

00:02:56.270 --> 00:02:57.950
when we plot the disparities,

00:02:57.950 --> 00:03:01.580
we can see that African Americans are over five times

00:03:01.580 --> 00:03:05.660
more likely to be falsely identified as our reference group.

00:03:05.660 --> 00:03:10.730
Hispanic and Caucasian groups as well are over two

00:03:10.730 --> 00:03:16.060
times more likely to be falsely identified than our Asian reference group.

00:03:16.060 --> 00:03:21.740
Hopefully this can help you see how tools like aequitas can help you identify bias in

00:03:21.740 --> 00:03:27.005
your model's predictions for different demographic groups and in particular,

00:03:27.005 --> 00:03:31.410
allow you to even choose key reference groups to consider.

