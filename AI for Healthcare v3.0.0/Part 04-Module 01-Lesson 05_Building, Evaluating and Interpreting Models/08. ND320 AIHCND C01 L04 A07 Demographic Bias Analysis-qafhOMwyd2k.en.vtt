WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.260
For this screencast, we will go through the following steps.

00:00:04.260 --> 00:00:09.840
First, we will select the groups which we will analyze and then we will

00:00:09.840 --> 00:00:15.885
prepare the data and finally analyze different metrics with the groups.

00:00:15.885 --> 00:00:18.720
Now, that you learned a little about aequitas

00:00:18.720 --> 00:00:22.020
and the importance of demographic bias analysis.

00:00:22.020 --> 00:00:25.260
Let's get to actual using the tool and go through

00:00:25.260 --> 00:00:31.095
those three steps for analyzing our dataset for demographic bias analysis.

00:00:31.095 --> 00:00:32.790
In this walk through,

00:00:32.790 --> 00:00:36.720
we will use the MPG model that we walked through earlier.

00:00:36.720 --> 00:00:38.235
In this case though,

00:00:38.235 --> 00:00:41.215
we'll think of demographics in terms of cars.

00:00:41.215 --> 00:00:46.600
First, we must select the field that we want to include in our analysis group.

00:00:46.600 --> 00:00:52.040
In this case, we will choose the origin field which is the country of origin for a car.

00:00:52.040 --> 00:00:58.700
In this part, I'm just adding the test data origin field to the output model results.

00:00:58.700 --> 00:01:01.630
You can see this example here.

00:01:01.630 --> 00:01:05.615
Now, we can move to actually preparing the data for the analysis.

00:01:05.615 --> 00:01:09.139
Aequitas provide some pandas preprocessing functions.

00:01:09.139 --> 00:01:10.790
Also for this step,

00:01:10.790 --> 00:01:15.050
we will fill null values as well with negative one as a placeholder value.

00:01:15.050 --> 00:01:18.000
This is largely the boilerplate code that

00:01:18.000 --> 00:01:20.885
they are provided for preparing data for analysis.

00:01:20.885 --> 00:01:26.275
But please look at the documentation for more details if you have any further questions.

00:01:26.275 --> 00:01:29.015
Now, we're ready to analyze the data.

00:01:29.015 --> 00:01:33.415
First, let's visualize the true positive rate or recall.

00:01:33.415 --> 00:01:36.510
For this function, we take the DataFrame from

00:01:36.510 --> 00:01:40.880
the preprocessing step and the TPR string as an argument,

00:01:40.880 --> 00:01:43.325
as well as an argument for including

00:01:43.325 --> 00:01:47.305
only groups with at least five percent or more of the data.

00:01:47.305 --> 00:01:53.930
In this case, you can see TPR being similar between the three countries of origin.

00:01:53.930 --> 00:01:59.545
So no apparent bias in the model results across the three countries for origin.

00:01:59.545 --> 00:02:04.280
Now, let's use a similar method to visualize the false positive rate.

00:02:04.280 --> 00:02:06.080
In this case, you can see that

00:02:06.080 --> 00:02:10.010
the false positive rate for Japan is much higher than other groups.

00:02:10.010 --> 00:02:12.410
Perhaps, there could be characteristics of

00:02:12.410 --> 00:02:16.525
cars from this country that are leading to this type of error.

00:02:16.525 --> 00:02:20.090
This visualization is more for flagging and identifying

00:02:20.090 --> 00:02:24.950
risks of key demographic groups that you're seeing in your prediction values.

00:02:24.950 --> 00:02:27.380
Demographic bias analysis on

00:02:27.380 --> 00:02:31.550
model predictions isn't exciting important part of building models,

00:02:31.550 --> 00:02:35.120
that represent and meet the needs of broader society.

00:02:35.120 --> 00:02:39.335
In this walk through, while you may not have dealt with patient demographics yet,

00:02:39.335 --> 00:02:41.540
you have learned some of the key methods in

00:02:41.540 --> 00:02:45.960
this exercise that will allow you to do this in the course.

