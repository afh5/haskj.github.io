WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.145
Nice job, taking a crack at building uncertainty estimation model.

00:00:05.145 --> 00:00:06.570
I know this is difficult,

00:00:06.570 --> 00:00:10.140
so don't feel like you have to understand every single part of it.

00:00:10.140 --> 00:00:12.945
So just to review the instructions,

00:00:12.945 --> 00:00:16.320
we use the Swiss heart disease dataset that we've been working with.

00:00:16.320 --> 00:00:17.850
I asked you to create

00:00:17.850 --> 00:00:22.950
an uncertainty estimation model that accounts for epistemic uncertainty as well.

00:00:22.950 --> 00:00:28.215
So provide this link to the TensorFlow tutorial that walks you through how to do this.

00:00:28.215 --> 00:00:31.870
I just asked you to provide the mean and standard deviation outputs.

00:00:31.870 --> 00:00:35.790
So this should be very similar to what we did in the walkthrough.

00:00:35.790 --> 00:00:41.170
So I provided you the posterior mean and prior distribution functions.

00:00:41.170 --> 00:00:44.720
These are largely functions that you can just use and

00:00:44.720 --> 00:00:48.290
plug and play into just one layer of that we had,

00:00:48.290 --> 00:00:50.590
and that's is DenseVariational layer,

00:00:50.590 --> 00:00:52.965
if you can see that was added here,

00:00:52.965 --> 00:00:55.400
and the difference is that instead of having

00:00:55.400 --> 00:00:57.830
a dense layer like we had in the walkthrough,

00:00:57.830 --> 00:00:59.765
we have a dense variational layer,

00:00:59.765 --> 00:01:04.330
which is where we can provide the posterior mean and prior trainable.

00:01:04.330 --> 00:01:07.205
At a high level, and I won't go into detail,

00:01:07.205 --> 00:01:10.160
but basically what these are is just think about what we

00:01:10.160 --> 00:01:13.160
went over with Bayesian probability of having a prior,

00:01:13.160 --> 00:01:15.715
and posterior, and having updates.

00:01:15.715 --> 00:01:18.590
So that's basically what these functions are doing,

00:01:18.590 --> 00:01:20.630
and they're doing this within this layer,

00:01:20.630 --> 00:01:23.275
a lot of the hard work is abstracted for you.

00:01:23.275 --> 00:01:29.060
Tensorflow probability is working to combine a lot of this information and reduce errors.

00:01:29.060 --> 00:01:33.055
But as I mentioned, there's a lot of iterations are continually going,

00:01:33.055 --> 00:01:35.940
so a lava documentation might be lacking.

00:01:35.940 --> 00:01:39.500
So don't get too frustrated if you don't understand or you can't read

00:01:39.500 --> 00:01:43.850
the documentation and find it as accessible as some other frameworks.

00:01:43.850 --> 00:01:45.555
So yeah, there you go.

00:01:45.555 --> 00:01:46.830
We have built the model.

00:01:46.830 --> 00:01:48.755
So definitely there's a lot of work.

00:01:48.755 --> 00:01:51.050
This is not a high-performing model.

00:01:51.050 --> 00:01:52.985
I only use two features,

00:01:52.985 --> 00:01:56.450
and that's mainly because of what I've done in the previous exercise,

00:01:56.450 --> 00:01:58.195
just to illustrate this.

00:01:58.195 --> 00:02:01.610
You can see that the model is not performing very well,

00:02:01.610 --> 00:02:05.090
but I believe this as a challenge to you to better iterate on

00:02:05.090 --> 00:02:09.160
this and see how you can improve this model and utilize this.

00:02:09.160 --> 00:02:14.240
Also, you can definitely take this model and sample

00:02:14.240 --> 00:02:19.235
across it for multiple predictions across that normal distribution.

00:02:19.235 --> 00:02:23.245
So here, you can see instead of y-hat, i have y-hats.

00:02:23.245 --> 00:02:26.810
Basically, I'm pulling from that normal distribution and getting a bunch of

00:02:26.810 --> 00:02:31.205
mean and standard deviation predictions for any given point.

00:02:31.205 --> 00:02:33.440
So there is a one prediction,

00:02:33.440 --> 00:02:36.680
there could be multiple means predictions.

00:02:36.680 --> 00:02:38.495
So this is just an example,

00:02:38.495 --> 00:02:40.505
and largely they are very similar,

00:02:40.505 --> 00:02:43.150
but in some cases you can see more variation.

00:02:43.150 --> 00:02:46.640
This is a nice tool that you can use where you might

00:02:46.640 --> 00:02:50.750
have datasets that are highly skewed in a lot of variability,

00:02:50.750 --> 00:02:55.550
and you'll be able to determine with greater confidence where the variances,

00:02:55.550 --> 00:02:57.010
where is it less certain?

00:02:57.010 --> 00:02:59.375
Because it's just as important to know

00:02:59.375 --> 00:03:02.825
what information you don't know and what you're less confident in,

00:03:02.825 --> 00:03:05.180
as it is to know what you do know.

00:03:05.180 --> 00:03:08.010
So one more thing before I wrap up,

00:03:08.010 --> 00:03:09.895
now that you have reviewed this,

00:03:09.895 --> 00:03:15.130
you can check out other ways to more dynamically account for variation in your dataset,

00:03:15.130 --> 00:03:16.990
where you might not know the pattern for

00:03:16.990 --> 00:03:22.120
standard deviation in cases where there might be ups and downs in your regression model.

00:03:22.120 --> 00:03:28.130
In our case, we just provide a standard scale that goes up linearly.

00:03:28.130 --> 00:03:31.285
But you may want to have something that is more dynamic,

00:03:31.285 --> 00:03:34.615
and so there's a layer called variational Gaussian process,

00:03:34.615 --> 00:03:36.985
which can be a lot more flexible to your needs.

00:03:36.985 --> 00:03:39.280
There's still more work at the time of

00:03:39.280 --> 00:03:42.190
writing to make this more accessible to most users.

00:03:42.190 --> 00:03:44.840
So I didn't want to cover that in detail.

00:03:44.840 --> 00:03:50.485
For now that you have built a TensorFlow probability model for epistemic uncertainty,

00:03:50.485 --> 00:03:54.529
you have a better idea of how to not only count for known unknowns,

00:03:54.529 --> 00:03:56.820
but also unknown unknowns.

00:03:56.820 --> 00:03:59.840
This can be helpful for different applications where you might

00:03:59.840 --> 00:04:03.580
have wider variance and wider distributions.

00:04:03.580 --> 00:04:06.860
Definitely, a lot more work to learn about this framework,

00:04:06.860 --> 00:04:09.320
but I'm glad you have the opportunity to learn more

00:04:09.320 --> 00:04:12.840
about it through this exercise. Thank you.

