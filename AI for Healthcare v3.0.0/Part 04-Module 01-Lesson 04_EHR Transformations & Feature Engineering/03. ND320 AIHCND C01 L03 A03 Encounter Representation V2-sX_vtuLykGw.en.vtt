WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.745
So what is an encounter?

00:00:02.745 --> 00:00:06.435
The definition of an encounter commonly used for HR records

00:00:06.435 --> 00:00:10.185
comes from the Health Level 7 International Organization,

00:00:10.185 --> 00:00:13.875
which sets the international standards for health care data.

00:00:13.875 --> 00:00:15.945
As the definition states,

00:00:15.945 --> 00:00:20.415
it is essentially an interaction between a patient and a health care professional.

00:00:20.415 --> 00:00:24.510
It usually refers to doctor visits and hospital stays.

00:00:24.510 --> 00:00:30.945
Now that we know what encounter is and also where it fits relative to other EHR levels,

00:00:30.945 --> 00:00:37.005
we are ready to go over how to convert a dataset from the line level to encounter level.

00:00:37.005 --> 00:00:40.075
So let's go through this notebook to see how to do this.

00:00:40.075 --> 00:00:41.840
However, before we do that,

00:00:41.840 --> 00:00:45.860
let's just talk you through the code that was used to make a synthetic dataset.

00:00:45.860 --> 00:00:49.699
Because of issues around using datasets with PHI,

00:00:49.699 --> 00:00:52.880
I created synthetic data set that we will use throughout

00:00:52.880 --> 00:00:57.650
the course that can show you the general structure of how EHR claims look like.

00:00:57.650 --> 00:01:00.830
There is a bit more complexity with the actual dataset.

00:01:00.830 --> 00:01:03.305
But for the purposes of this course,

00:01:03.305 --> 00:01:06.680
this should help us understand the general concepts so

00:01:06.680 --> 00:01:09.965
that when you work with these types of datasets in real life,

00:01:09.965 --> 00:01:11.540
it is familiar to you.

00:01:11.540 --> 00:01:13.935
For the actual synthetic dataset,

00:01:13.935 --> 00:01:22.639
I created it with 100,000 records with 7,800 encounters and 1,000 patients.

00:01:22.639 --> 00:01:25.460
I use a normal distribution for randomly selecting

00:01:25.460 --> 00:01:28.280
values and made sure that key fields like,

00:01:28.280 --> 00:01:32.885
principal diagnosis code are duplicated across encounter lines.

00:01:32.885 --> 00:01:35.360
I'm just showing you in case you want and play around

00:01:35.360 --> 00:01:37.700
with creating different sized data sets.

00:01:37.700 --> 00:01:40.550
It's not important for you to know the step-by-step here.

00:01:40.550 --> 00:01:44.030
This was done in a quick ad hoc fashion to enable us to use

00:01:44.030 --> 00:01:48.745
a dataset that is similar to what I've worked on in the real world.

00:01:48.745 --> 00:01:52.930
Now that we know a little bit about the synthetic dataset that we will be using,

00:01:52.930 --> 00:01:57.515
we can move on to lesson activity of converting from line to encounter level.

00:01:57.515 --> 00:02:02.965
So let's load up the synthetic dataset and take a quick look at a random encounter.

00:02:02.965 --> 00:02:09.095
For ID 100, you can see that is all for the same patient 585 and that

00:02:09.095 --> 00:02:17.140
the principle diagnosis code is 71465 and is repeated across the lines.

00:02:17.140 --> 00:02:22.255
The key thing to note is that the each row is different for the code set columns,

00:02:22.255 --> 00:02:25.450
medication, lab code, and procedure code.

00:02:25.450 --> 00:02:31.030
Hopefully, this illustrates how this encounter dataset is denormalized at the line level.

00:02:31.030 --> 00:02:34.015
Now we can do the actual data transformation

00:02:34.015 --> 00:02:37.030
to convert this dataset to the encounter level.

00:02:37.030 --> 00:02:39.580
First, in this code block,

00:02:39.580 --> 00:02:43.060
I create a column lists for the fields we're using to group,

00:02:43.060 --> 00:02:45.970
and then the fields not included in the grouping.

00:02:45.970 --> 00:02:48.490
Then in this code block,

00:02:48.490 --> 00:02:51.175
I just use the pandas group by method in

00:02:51.175 --> 00:02:56.990
aggregate function to build this dataset and only include values that are not null.

00:02:56.990 --> 00:02:58.620
For real life data sets,

00:02:58.620 --> 00:03:00.840
I might use a big data SQL engine,

00:03:00.840 --> 00:03:04.150
or Spark to convert and prepare a dataset.

00:03:04.150 --> 00:03:07.400
But the general aggregation logic would be similar.

00:03:07.400 --> 00:03:10.135
So now that we've transformed this data,

00:03:10.135 --> 00:03:16.025
we can look at this output for what we created with an encounter level dataset.

00:03:16.025 --> 00:03:19.630
Let's just take a look at five records here.

00:03:19.630 --> 00:03:21.760
If we see this output here,

00:03:21.760 --> 00:03:24.865
we can take this first patient_ID 186,

00:03:24.865 --> 00:03:28.540
and just select the first encounter for that patient.

00:03:28.540 --> 00:03:33.460
We will use this encounter to compare the output from when we

00:03:33.460 --> 00:03:38.380
created the encounter level dataset to the input of the line level dataset.

00:03:38.380 --> 00:03:41.185
I'll show you in a moment what that looks like.

00:03:41.185 --> 00:03:44.830
So here's a look at the line level for encounter_id_1.

00:03:44.830 --> 00:03:47.635
You can see encounter_id_1 being

00:03:47.635 --> 00:03:51.805
duplicated and the patient_ID being duplicated across the rows,

00:03:51.805 --> 00:03:54.025
as well as the principle diagnosis code.

00:03:54.025 --> 00:03:56.815
However, as we learned earlier,

00:03:56.815 --> 00:03:59.290
the procedure code, medication code,

00:03:59.290 --> 00:04:04.690
lab codes will be distinct across the rows or could be distinct depending on the case.

00:04:04.690 --> 00:04:07.120
So this is the source of where things are different

00:04:07.120 --> 00:04:10.535
across the rows and this is the line level view.

00:04:10.535 --> 00:04:17.165
Then, let's take a look of how this is aggregated up into that encounter level view.

00:04:17.165 --> 00:04:20.660
It's basically all of this information being squashed

00:04:20.660 --> 00:04:24.815
and aggregated up together into array here,

00:04:24.815 --> 00:04:26.330
and here, and here.

00:04:26.330 --> 00:04:28.430
So that is an example of how you take

00:04:28.430 --> 00:04:33.300
a line level dataset and convert it to an encounter level dataset.

