WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.570
Now that we have discussed how convolutional neural network

00:00:03.570 --> 00:00:07.500
operate and we discussed different types of convolutions that you might be dealing with,

00:00:07.500 --> 00:00:09.480
I invite you to join me and do

00:00:09.480 --> 00:00:13.275
a little exercise which will let you practice with those convolutions yourself.

00:00:13.275 --> 00:00:16.320
Welcome to a screencast of a convolutions notebook.

00:00:16.320 --> 00:00:18.435
This is an introduction into an exercise.

00:00:18.435 --> 00:00:22.080
First part of this notebook will get you set up with some of

00:00:22.080 --> 00:00:24.150
the mechanics behind convolutions and how

00:00:24.150 --> 00:00:26.520
we'll be doing these convolutions for this exercise,

00:00:26.520 --> 00:00:29.090
and the rest of this notebook will invite you

00:00:29.090 --> 00:00:31.990
to write some of the convolutional code on your own.

00:00:31.990 --> 00:00:35.540
I would like to walk you through the beginning of this exercise because I

00:00:35.540 --> 00:00:39.635
think some things that we're touching upon here are worth talking through.

00:00:39.635 --> 00:00:42.530
Let's see the collection of libraries that we're dealing with.

00:00:42.530 --> 00:00:43.860
We have the Pydicom here,

00:00:43.860 --> 00:00:49.870
the usual suspect in dealing with Python and medical images. We have the Matplotlib.

00:00:49.870 --> 00:00:53.365
We have the NumPy. Also, we have the PyTorch here.

00:00:53.365 --> 00:00:58.110
So PyTorch is a library for building convolutional neural networks and other types of

00:00:58.110 --> 00:01:00.350
neural networks and PyTorch has a lot of

00:01:00.350 --> 00:01:03.485
convenience functions for various computer vision tasks.

00:01:03.485 --> 00:01:08.180
In particular, PyTorch has definitions for convolutional layers that you would

00:01:08.180 --> 00:01:10.550
build into your convolutional neural networks

00:01:10.550 --> 00:01:13.370
and we will use those layers to compute convolutions.

00:01:13.370 --> 00:01:16.700
As we have discussed, convolutions involve moving this window onto

00:01:16.700 --> 00:01:19.010
a convolutional kernel and computing

00:01:19.010 --> 00:01:21.775
the values and copying these values onto a new image.

00:01:21.775 --> 00:01:24.590
PyTorch has implementation of convolutional layers,

00:01:24.590 --> 00:01:26.210
they take care of all this mechanics.

00:01:26.210 --> 00:01:29.795
That's why we won't have to deal with all this math ourselves.

00:01:29.795 --> 00:01:34.565
We'll use another library here which is called NiBabel.

00:01:34.565 --> 00:01:38.950
This is a library which is used to load NifTI files.

00:01:38.950 --> 00:01:41.850
You will work with this library quite a bit as well.

00:01:41.850 --> 00:01:44.300
We will use this library called Pillow,

00:01:44.300 --> 00:01:46.700
which is the Python library for one of

00:01:46.700 --> 00:01:51.020
the many Python libraries for working with regular two-dimensional images.

00:01:51.020 --> 00:01:54.765
We will be loading a bitmap in here.

00:01:54.765 --> 00:01:56.520
So let's move on.

00:01:56.520 --> 00:02:01.295
So let us load all those libraries and let us talk about applying convolutional filters.

00:02:01.295 --> 00:02:06.980
First, let us define a four by four convolutional kernel and let's print this matrix.

00:02:06.980 --> 00:02:08.510
We define an edge filter.

00:02:08.510 --> 00:02:13.730
This is a convolutional kernel that will multiply values by one at one side of

00:02:13.730 --> 00:02:16.400
the matrix and multiply the values by

00:02:16.400 --> 00:02:19.735
minus one on another side of the matrix and combine them.

00:02:19.735 --> 00:02:24.740
This is a quite typical edge filter and we will see how it works.

00:02:24.740 --> 00:02:27.485
Let us define a convolutional layer.

00:02:27.485 --> 00:02:31.160
We're using this Conv2d class from

00:02:31.160 --> 00:02:36.815
the PyTorch's neural network module and this Convd2d uses a few input parameters.

00:02:36.815 --> 00:02:39.830
So first, we need to input the input size,

00:02:39.830 --> 00:02:41.765
the number of channels that you're dealing with.

00:02:41.765 --> 00:02:43.270
For the purpose of this exercise,

00:02:43.270 --> 00:02:44.450
of this little code snippet,

00:02:44.450 --> 00:02:46.040
we will have just one channel.

00:02:46.040 --> 00:02:48.050
We're specifying the output size, again,

00:02:48.050 --> 00:02:49.550
one channel for the kernel size,

00:02:49.550 --> 00:02:51.660
which is four by four, and the bias.

00:02:51.660 --> 00:02:56.695
We will not be using biases here because we're just focusing on the weights.

00:02:56.695 --> 00:02:59.070
Let's set up this layer and let's see what we get.

00:02:59.070 --> 00:03:02.370
So PyTorch can print the layer in a readable format.

00:03:02.370 --> 00:03:04.885
So we have some values here. It's looking good.

00:03:04.885 --> 00:03:06.830
I encourage you as you will be going through

00:03:06.830 --> 00:03:09.860
this exercise to go and read up the PyTorch's documentation.

00:03:09.860 --> 00:03:11.225
The documentation is pretty good.

00:03:11.225 --> 00:03:13.970
It explains what exactly those values are.

00:03:13.970 --> 00:03:19.315
It is quite important to be precise about the dimensions of your input data.

00:03:19.315 --> 00:03:24.970
Next, let's turn our convolutional kernel into a PyTorch tensor.

00:03:24.970 --> 00:03:26.800
PyTorch operates on tensors.

00:03:26.800 --> 00:03:29.750
So everything needs to be turned into a PyTorch tensor.

00:03:29.750 --> 00:03:31.930
PyTorch conveniently has a lot of converters,

00:03:31.930 --> 00:03:34.490
so we're just using this from NumPy methods,

00:03:34.490 --> 00:03:38.075
which converts our NumPy array into a PyTorch tensor.

00:03:38.075 --> 00:03:39.680
A couple of other things to note here.

00:03:39.680 --> 00:03:42.240
We convert it into a float tensor to make

00:03:42.240 --> 00:03:45.135
sure that it's float and we apply this unsqueeze function.

00:03:45.135 --> 00:03:48.410
Unsqueeze function basically adds dimensions to our array.

00:03:48.410 --> 00:03:51.950
Our array is a two by two array and

00:03:51.950 --> 00:03:56.790
PyTorch express your parameter tensor to be four-dimensional.

00:03:56.790 --> 00:04:00.410
So basically, the first dimension is the number of output channels,

00:04:00.410 --> 00:04:02.240
the second dimension, number of input channels,

00:04:02.240 --> 00:04:04.880
and then you have your kernel X and Y dimensions.

00:04:04.880 --> 00:04:07.535
That's why we need the four-dimensional tensor and

00:04:07.535 --> 00:04:10.610
we are not really using this output-input channel dimension.

00:04:10.610 --> 00:04:14.450
So we can just not store anything there,

00:04:14.450 --> 00:04:17.915
but we still need a four-dimensional tensor.

00:04:17.915 --> 00:04:21.440
Then what we do, we just turn this parameters tensor into

00:04:21.440 --> 00:04:24.170
a torch parameter and we use this parameter

00:04:24.170 --> 00:04:28.190
to initialize our convolutional layer weights.

00:04:28.460 --> 00:04:31.565
To make sure that we got our dimensions right,

00:04:31.565 --> 00:04:36.590
let us print the shape out of our parameters tensor to make sure that it is correct.

00:04:36.590 --> 00:04:38.180
We see it's a four-dimensional tensor.

00:04:38.180 --> 00:04:43.195
We have one by one by four by four four-dimensional tensor.

00:04:43.195 --> 00:04:45.975
Let's keep going. Let's load an image.

00:04:45.975 --> 00:04:49.220
It's an image of a walrus in a aquarium,

00:04:49.220 --> 00:04:50.960
which was playing with some kids,

00:04:50.960 --> 00:04:55.905
and I enjoyed watching this walrus playing with the kids.

00:04:55.905 --> 00:05:00.065
Let's convert our walrus into a grayscale image

00:05:00.065 --> 00:05:04.440
because that's what's PyTorch is expecting.

00:05:04.440 --> 00:05:07.140
So remember, we have one channel as an input.

00:05:07.140 --> 00:05:12.965
We need to have something where the values are located on a linear scale.

00:05:12.965 --> 00:05:17.810
So we convert the integer data types that we have in the image

00:05:17.810 --> 00:05:22.790
that we've loaded into singles and we scale it to the range of 0-1.

00:05:22.790 --> 00:05:24.445
That's what we get as an output.

00:05:24.445 --> 00:05:30.050
At this point, let's convert this image into a PyTorch tensor.

00:05:30.050 --> 00:05:31.910
Again, we apply the same operation.

00:05:31.910 --> 00:05:35.480
We convert this image into a tensor by using

00:05:35.480 --> 00:05:38.210
the from_numpy function and we apply

00:05:38.210 --> 00:05:42.260
the unsqueeze operation to turn it into a four-dimensional tensor.

00:05:42.260 --> 00:05:48.330
That's what we get. We have one by one by 768 by 1054 image At this point,

00:05:48.330 --> 00:05:50.210
it's fairly easy to apply the convolutions.

00:05:50.210 --> 00:05:53.180
We have our convolutional layer and we just call

00:05:53.180 --> 00:05:56.995
this convolutional layer on the walrus tensor variable.

00:05:56.995 --> 00:06:02.420
Let's also compute the rectified linear unit operation

00:06:02.420 --> 00:06:05.375
on the tensor that we get as the output.

00:06:05.375 --> 00:06:07.910
As you might remember, convolutional neural networks,

00:06:07.910 --> 00:06:09.920
they compute activation functions.

00:06:09.920 --> 00:06:11.630
So after you have your convolutions,

00:06:11.630 --> 00:06:13.745
you would like to have an activation function to

00:06:13.745 --> 00:06:16.730
improve your networks capability to generalize.

00:06:16.730 --> 00:06:19.040
Let's see what our convolutions looks like and let's

00:06:19.040 --> 00:06:21.770
see what our rectified linear unit look like.

00:06:21.770 --> 00:06:23.700
Also, take a note here,

00:06:23.700 --> 00:06:26.270
we have this magic function called time,

00:06:26.270 --> 00:06:29.075
which let's us measure how much time our operation takes.

00:06:29.075 --> 00:06:33.625
On my laptop here, it takes 63 milliseconds, and let's visualize.

00:06:33.625 --> 00:06:36.660
So let's visualize our convolved walrus.

00:06:36.660 --> 00:06:38.835
So we can see that edges turned out here.

00:06:38.835 --> 00:06:40.070
We're applying edge filters,

00:06:40.070 --> 00:06:42.185
so things seem to be checking out.

00:06:42.185 --> 00:06:48.410
Let's visualize the rectified linear units activation function.

00:06:48.410 --> 00:06:50.990
We can see, if you remember, what's a rally layer does,

00:06:50.990 --> 00:06:53.375
it basically shaves off all the negative values,

00:06:53.375 --> 00:06:56.600
and all the positive values,

00:06:56.600 --> 00:06:58.490
they get mapped one on one to linear scale.

00:06:58.490 --> 00:07:00.090
So you can see what actually

00:07:00.090 --> 00:07:05.230
the rally layer in a real neural network would do with this image.

00:07:05.230 --> 00:07:09.600
So another quick note on working with a medical imaging volume,

00:07:09.600 --> 00:07:13.040
because this is what I propose you to do in this exercise.

00:07:13.040 --> 00:07:14.510
You'll be applying 2D,

00:07:14.510 --> 00:07:17.525
2.5D and 3D convolutions to a 3D volume.

00:07:17.525 --> 00:07:20.840
Right now, we've done it for a two-dimensional image.

00:07:20.840 --> 00:07:24.310
So let's do it for a medical imaging volume.

00:07:24.310 --> 00:07:28.290
We will use the NiBabel library to load a NifTI file.

00:07:28.290 --> 00:07:32.050
NifTI files, they come with this.nii.gz extension.

00:07:32.050 --> 00:07:34.130
Let's do that, and this NiBabel library,

00:07:34.130 --> 00:07:36.710
it has this function called get_fdata,

00:07:36.710 --> 00:07:39.635
which converts the pixels into a NumPy array.

00:07:39.635 --> 00:07:42.050
So let's see what we're dealing with here.

00:07:42.050 --> 00:07:48.230
We're dealing with image that has 512 by 512 in plane resolution and has 41 slices.

00:07:48.230 --> 00:07:51.295
Let's visualize and see what we're dealing with here.

00:07:51.295 --> 00:07:53.820
So last dimension here, well,

00:07:53.820 --> 00:07:57.590
I could tell that because I know that it's a CT image.

00:07:57.590 --> 00:08:02.870
Typically, CT images, they have isotropic voxels on the in-plane.

00:08:02.870 --> 00:08:07.850
So I could have guessed that this last dimension was the z-dimension, and it is.

00:08:07.850 --> 00:08:09.540
It's a abdominal crosscut.

00:08:09.540 --> 00:08:14.590
So let us look at the slice number 0 and let us look at the slice number 40.

00:08:14.590 --> 00:08:17.730
So it seems to be some cut through the abdomen.

00:08:17.730 --> 00:08:20.640
So this slice number 40 sits at the top here.

00:08:20.640 --> 00:08:23.115
We have lungs here. We have a portion of heart here.

00:08:23.115 --> 00:08:25.680
Item slice number 0 sits probably somewhere

00:08:25.680 --> 00:08:28.825
at the bottom where we have all these different values here.

00:08:28.825 --> 00:08:33.315
Let us visualize the sagittal cross-section of this volume.

00:08:33.315 --> 00:08:36.450
So let us do our own little MPR exercise,

00:08:36.450 --> 00:08:40.390
similar to what you have done in the previous lesson.

00:08:40.390 --> 00:08:42.825
We see the super squished image.

00:08:42.825 --> 00:08:45.770
By now, this shouldn't be a surprise for us because we're looking at an image

00:08:45.770 --> 00:08:48.950
which is 512 pixels across and just 40 pixels down.

00:08:48.950 --> 00:08:51.760
Obviously, this is not what the real anatomy looks like.

00:08:51.760 --> 00:08:55.620
So remember, our non-isotropic pixels,

00:08:55.620 --> 00:08:58.320
remember how in one of the previous exercises,

00:08:58.320 --> 00:09:01.940
we looked at DICOM pixel

00:09:01.940 --> 00:09:06.185
spacing and slice thickness parameters to compute the aspect ratio that we should apply.

00:09:06.185 --> 00:09:08.450
Right now, we're dealing with NifTI images.

00:09:08.450 --> 00:09:10.885
So if you remember the conversation about NifTI,

00:09:10.885 --> 00:09:13.670
NifTI has this field called pixdim and this is

00:09:13.670 --> 00:09:16.850
the field that stores the information about pixel dimensions.

00:09:16.850 --> 00:09:19.610
Let's look at the pixdim field and let's see what is

00:09:19.610 --> 00:09:22.820
the aspect ratio that we should apply here.

00:09:22.820 --> 00:09:25.450
So without going into too much detail,

00:09:25.450 --> 00:09:27.655
if you're curious, I have a link here.

00:09:27.655 --> 00:09:30.925
You can go and read up on this pixdim variable.

00:09:30.925 --> 00:09:36.940
But basically, I will tell you that the fields that are in the positions 1,

00:09:36.940 --> 00:09:40.425
2, and 3 are our three pixel dimensions.

00:09:40.425 --> 00:09:44.100
So we have this 0.74 by 0.74 by 5 pixels.

00:09:44.100 --> 00:09:46.420
So we have five millimeter slice thickness.

00:09:46.420 --> 00:09:50.860
From there, I think you can guess how we should compute our aspect ratio.

00:09:50.860 --> 00:09:52.695
Let's do that. We get our image.

00:09:52.695 --> 00:09:56.330
Of course, our z-axis resolution is not too great.

00:09:56.330 --> 00:09:58.885
So we see that the image is very pixelated.

00:09:58.885 --> 00:10:00.970
One last thing that I'm going to show you

00:10:00.970 --> 00:10:03.580
is I'm going to give you a convenience function,

00:10:03.580 --> 00:10:07.895
which will display several slices through the volume on a nice grid,

00:10:07.895 --> 00:10:11.210
similar to how we did it in one of the previous screencasts.

00:10:11.210 --> 00:10:13.985
Let's do that and let's visualize some of the slices.

00:10:13.985 --> 00:10:16.160
Well, actually, all of this slices in this image,

00:10:16.160 --> 00:10:17.965
and this is what we're looking at.

00:10:17.965 --> 00:10:21.110
So we have gone through the introductory section of the exercise.

00:10:21.110 --> 00:10:24.125
You're welcome to take a closer look at all the operations that we did,

00:10:24.125 --> 00:10:27.875
especially at how we converted NumPy arrays into PyTorch tensors,

00:10:27.875 --> 00:10:32.605
because you will be doing that a lot as you are building your own neural networks.

00:10:32.605 --> 00:10:37.355
Now, I invite you to go and continue working through this notebook,

00:10:37.355 --> 00:10:41.075
where I'm suggesting that you should compute 2D, 3D,

00:10:41.075 --> 00:10:45.570
and 2.5D convolutions from this medical imaging volume.

