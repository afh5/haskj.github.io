WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.335
So in this video, we will talk about performance evaluation of segmentation algorithms.

00:00:04.335 --> 00:00:07.365
As we have seen in the exercise that we have done,

00:00:07.365 --> 00:00:08.940
we used Cross-Entropy loss,

00:00:08.940 --> 00:00:11.280
and Cross-Entropy loss can work pretty well.

00:00:11.280 --> 00:00:15.090
This is a classic metric for clarification project.

00:00:15.090 --> 00:00:18.300
Although Cross-Entropy loss tends to favor the dominant class,

00:00:18.300 --> 00:00:21.955
especially if your class has become highly imbalanced.

00:00:21.955 --> 00:00:24.240
For example, if you haven't done that,

00:00:24.240 --> 00:00:25.830
I invite you to come back to

00:00:25.830 --> 00:00:30.960
the exercise and try to increase the field of view or try to throw in slices

00:00:30.960 --> 00:00:35.660
which do not have any ground truth segmentations on them and see how it

00:00:35.660 --> 00:00:41.045
changes the tendency for your network to under-segment the structure.

00:00:41.045 --> 00:00:46.744
So we will talk about some of the other metrics that you can use for segmentation,

00:00:46.744 --> 00:00:48.170
things like Dice, Jaccard,

00:00:48.170 --> 00:00:50.495
and the Hausdorff distance.

00:00:50.495 --> 00:00:54.320
So first, a few words on metrics which are really

00:00:54.320 --> 00:00:58.220
popular in statistical performance evaluation; sensitivity and specificity.

00:00:58.220 --> 00:01:00.470
This metrics could also be used for

00:01:00.470 --> 00:01:02.990
evaluating the performance of segmentation algorithms.

00:01:02.990 --> 00:01:06.440
They're fairly easy to compute because you just

00:01:06.440 --> 00:01:10.040
need to count the number of voxels that you predict correctly and

00:01:10.040 --> 00:01:13.160
compare them to the number of voxels that are

00:01:13.160 --> 00:01:17.885
labeled as your structures or background in the ground truth.

00:01:17.885 --> 00:01:21.170
These metrics are easily relatable to clinicians

00:01:21.170 --> 00:01:24.560
because as we will see further in the lectures,

00:01:24.560 --> 00:01:28.250
clinicians operate in terms of sensitivity and specificity of

00:01:28.250 --> 00:01:32.885
tests quite often, and basically, sensitivity,

00:01:32.885 --> 00:01:35.570
which is a measure of a true positive rate,

00:01:35.570 --> 00:01:41.405
and it basically means the tendency of your model to under-segment structure,

00:01:41.405 --> 00:01:44.000
while specificity, which is a measure of true negative rate,

00:01:44.000 --> 00:01:48.470
it defines the tendency of your model two over-segment structures.

00:01:48.470 --> 00:01:53.300
But the problem with specificity and sensitivity is that specificity relies on

00:01:53.300 --> 00:01:59.225
the count of true negatives in your segmentation and in your ground truth,

00:01:59.225 --> 00:02:03.694
and that count relies upon having information about all the population,

00:02:03.694 --> 00:02:06.785
or in our case, all the background voxels.

00:02:06.785 --> 00:02:10.340
Therefore, the specificity metric tends to favor

00:02:10.340 --> 00:02:15.075
larger segments and tends to penalize smaller segments more than larger segments.

00:02:15.075 --> 00:02:18.455
These metrics only make sense when they come together,

00:02:18.455 --> 00:02:21.990
because if you have 100 percent sensitivity, it may mean a few things.

00:02:21.990 --> 00:02:25.420
It may mean that your model has very few false negatives or it may

00:02:25.420 --> 00:02:29.405
mean that your model just does not predict any negatives at all.

00:02:29.405 --> 00:02:35.035
So sensitivity and specificity only makes sense when they're presented together.

00:02:35.035 --> 00:02:37.040
Therefore, because of this problem,

00:02:37.040 --> 00:02:39.760
there are a few other metrics that people have thought about.

00:02:39.760 --> 00:02:42.380
So one metric which could address some of the problems that we

00:02:42.380 --> 00:02:45.275
have just discussed is called dissimilarity coefficient.

00:02:45.275 --> 00:02:48.455
It has been developed in the four years by two botanists,

00:02:48.455 --> 00:02:50.950
Thorvald Sorensen and Lee Raymond Dice.

00:02:50.950 --> 00:02:54.275
This metric could be computed by taking the intersection

00:02:54.275 --> 00:02:58.220
of two volumes and dividing that by their sum.

00:02:58.220 --> 00:03:01.580
This metric is fairly easy to compute because you just need to

00:03:01.580 --> 00:03:04.460
add up all the pixels and you just need

00:03:04.460 --> 00:03:07.110
to know what pixels belong to

00:03:07.110 --> 00:03:09.990
your prediction and what pixels belongs to your ground truth.

00:03:09.990 --> 00:03:11.915
This metric is fairly popular choice

00:03:11.915 --> 00:03:15.260
for measuring performance of segmentation algorithms.

00:03:15.260 --> 00:03:17.120
One thing to watch out in this metric,

00:03:17.120 --> 00:03:22.345
that your gradients may become unstable if you have very small values in the denominator.

00:03:22.345 --> 00:03:27.495
So you need to watch out for potentially the problem of exploding gradients.

00:03:27.495 --> 00:03:31.550
Another metric which is also quite popular for measuring the performance of

00:03:31.550 --> 00:03:33.680
segmentation algorithms and which has been used in

00:03:33.680 --> 00:03:36.170
the literature is called Jaccard Index.

00:03:36.170 --> 00:03:39.169
Jaccard Index is computed by dividing the intersection

00:03:39.169 --> 00:03:42.900
of two volumes by the union of the two volumes.

00:03:42.900 --> 00:03:46.280
Actually, this metric is no different from Dice coefficient because it's

00:03:46.280 --> 00:03:49.970
measuring the same thing and one can be easily derived from the other.

00:03:49.970 --> 00:03:53.000
So it does not really make sense to look at both

00:03:53.000 --> 00:03:56.900
because one does not give you any new information compared to the other.

00:03:56.900 --> 00:03:59.600
At the same time, the values that are produced are a little bit

00:03:59.600 --> 00:04:03.710
different and some people prefer looking at one over the other.

00:04:03.710 --> 00:04:06.785
The problem with both Dice metric and

00:04:06.785 --> 00:04:10.445
Jaccard metric can be illustrated by the following example.

00:04:10.445 --> 00:04:13.570
So let us look at this structure here.

00:04:13.570 --> 00:04:17.570
Let's assume that this is a square with sides 1,

00:04:17.570 --> 00:04:19.310
and you have another structure,

00:04:19.310 --> 00:04:20.540
this is your prediction,

00:04:20.540 --> 00:04:22.370
a square of the size 1.1.

00:04:22.370 --> 00:04:24.890
So if we compute the dice similarity score,

00:04:24.890 --> 00:04:27.625
we get the dissimilarity score of 90 percent here.

00:04:27.625 --> 00:04:29.430
Let us look at another example.

00:04:29.430 --> 00:04:32.330
Let's assume so you have the same ground-truth structure,

00:04:32.330 --> 00:04:33.980
which is a square with a side of 1,

00:04:33.980 --> 00:04:36.755
and now your prediction looks like this.

00:04:36.755 --> 00:04:40.250
So you have a section which overlaps very precisely on top of

00:04:40.250 --> 00:04:45.260
the ground truth and you have this offshoot that goes off to the sides,

00:04:45.260 --> 00:04:47.110
which is long, but very narrow.

00:04:47.110 --> 00:04:51.780
Well, guess what? Your dissimilarity score could be exactly the same.

00:04:51.780 --> 00:04:55.100
Therefore, dissimilarity score does not

00:04:55.100 --> 00:04:59.105
necessarily tell you how exactly structures are different.

00:04:59.105 --> 00:05:01.760
So 90 percent of dissimilarity score can mean,

00:05:01.760 --> 00:05:04.675
as we see in this example, very different things.

00:05:04.675 --> 00:05:07.120
Sometimes this is not a big problem.

00:05:07.120 --> 00:05:10.370
If you know that your model does not tend to segment

00:05:10.370 --> 00:05:13.805
this narrow structures or your ground truth does not have those.

00:05:13.805 --> 00:05:15.260
Sometimes where you're measuring

00:05:15.260 --> 00:05:18.940
highly irregular shapes or things like vascular structures,

00:05:18.940 --> 00:05:24.885
your Dice score might not be the best metric to measure the performance.

00:05:24.885 --> 00:05:28.695
There are some tricks to address that and

00:05:28.695 --> 00:05:32.845
one such trick is a metric which is called Hausdorff distance.

00:05:32.845 --> 00:05:36.380
Hausdorff distance is a metric which is defined by this,

00:05:36.380 --> 00:05:40.500
what may seem to be a complex mathematical expression, but basically,

00:05:40.500 --> 00:05:44.970
what it means is that we have two structures here that we're comparing,

00:05:44.970 --> 00:05:48.075
our set X and set Y.

00:05:48.075 --> 00:05:51.695
What we do, we look at every point on the boundary of

00:05:51.695 --> 00:05:55.475
x and we compute the distance to Y from every point,

00:05:55.475 --> 00:05:59.745
and we find the point which has the maximum distance to Y.

00:05:59.745 --> 00:06:04.880
Then we do the same thing, but looking it from the Y's edge perspective.

00:06:04.880 --> 00:06:07.640
So we go through every points of the set Y and

00:06:07.640 --> 00:06:10.420
we find the one which is furthest from the X.

00:06:10.420 --> 00:06:12.865
We take the maximum of those two values,

00:06:12.865 --> 00:06:15.080
and these two values, as this example illustrates,

00:06:15.080 --> 00:06:16.280
might not be the same.

00:06:16.280 --> 00:06:18.760
For example, this value from the set X,

00:06:18.760 --> 00:06:21.560
if we look from the Y's perspective,

00:06:21.560 --> 00:06:22.940
where we sit on this point,

00:06:22.940 --> 00:06:26.255
the shortest distance to X might be here, probably,

00:06:26.255 --> 00:06:31.310
but if we look from X perspective and if we go into this trough here,

00:06:31.310 --> 00:06:34.590
we will see that it's the farthest Y, and similar here.

00:06:34.590 --> 00:06:37.640
So Hausdorff Distance actually addresses this problem

00:06:37.640 --> 00:06:41.315
that we just discussed with dissimilarity coefficient.

00:06:41.315 --> 00:06:46.550
Hausdorff Distance penalizes when your model tends to either create

00:06:46.550 --> 00:06:49.685
those long half-shoots or

00:06:49.685 --> 00:06:51.980
the ground truth structures are highly

00:06:51.980 --> 00:06:54.670
irregular and the model is not exactly capturing that.

00:06:54.670 --> 00:06:59.880
The problem with Hausdorff Distance is that it is not that easy to compute.

00:06:59.880 --> 00:07:02.060
You basically need to evaluate

00:07:02.060 --> 00:07:05.990
all the distances between two sets and when you get into 3D,

00:07:05.990 --> 00:07:09.140
it gets particularly intensive.

00:07:09.140 --> 00:07:14.180
Something that people do is computing Mean Slice-Wise Hausdorff Distance.

00:07:14.180 --> 00:07:17.420
So basically, you compute a two-dimensional Hausdorff Distance

00:07:17.420 --> 00:07:21.145
for every slice in your segmentation and then you take the mean of that.

00:07:21.145 --> 00:07:25.410
So in this section, we have talked about several metrics for segmentation,

00:07:25.410 --> 00:07:26.720
we have talked about Dice Score,

00:07:26.720 --> 00:07:28.550
we have talked about Hausdorff Distance.

00:07:28.550 --> 00:07:30.525
One of the key takeaways is that,

00:07:30.525 --> 00:07:33.350
there is no one metric which is perfect and you need to

00:07:33.350 --> 00:07:36.065
understand where the ground truth is coming from,

00:07:36.065 --> 00:07:39.785
and what are the requirements and performance constraints of your algorithm,

00:07:39.785 --> 00:07:43.010
and what is the clinical environment that the algorithm will be operating on.

00:07:43.010 --> 00:07:44.520
In the next section,

00:07:44.520 --> 00:07:47.990
Martin will join us and talk about the clinical performance evaluation of

00:07:47.990 --> 00:07:51.710
diagnostic tests and other automated decision-making systems,

00:07:51.710 --> 00:07:54.290
and you will see what it means for

00:07:54.290 --> 00:07:58.560
a clinician when you present your performance relation characteristics to them.

